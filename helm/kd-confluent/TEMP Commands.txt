# K8s dashboard and token
http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/
kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | sls admin-user | ForEach-Object { $_ -Split '\s+' } | Select -First 1)

# Confluent Control Center port forward and UI
kubectl port-forward pod/controlcenter-0  9021:9021 -n kd-confluent
http://localhost:9021

# Kibana port forward and UI
kubectl port-forward svc/kibana-kibana 5601
http://localhost:5601

# Get logs from Connector container
docker logs c919509dc787d0a0e8a585fcfdc909cc670e3d1988f9a9421a41a7f24a5f71ec 2>&1 >kdccc.log
docker logs 1ad1de2bec4351c192abf3e52f6899cc6b4cd59a2fad539cb6fbd6950bb08303 2>&1 >kdkfk.log

# Internal Kafkacat within kd-confluent namespace - list topics, produce and consume messages
kafkacat -b kafka:9071 -L -F /etc/kafka-client-properties/kafka-client.properties
kafkacat -b kafka:9071 -t qe -P -F /etc/kafka-client-properties/kafka-client.properties
kafkacat -b kafka:9071 -t qe -C -F /etc/kafka-client-properties/kafka-client.properties

# Produce with key. So input <key>:<value>
kafkacat -b kafka:9071 -t qe -P -F /etc/kafka-client-properties/kafka-client.properties

# Internal Kafkacat outside kd-confluent namespace - list topics, produce and consume messages
kafkacat -b kafka.kd-confluent.svc.cluster.local:9071 -L -F /etc/kafka-client-properties/kafka-client.properties

# External Kafkacat
kafkacat -b 192.168.65.3:30220 -L -F kafka.properties

# On Mosquitto container - publish and consume messages
mosquitto_pub -h kd-mosquitto -t temperature -m "good morning"
mosquitto_sub -h kd-mosquitto -t temperature -C 5

# Configure Datagen connector
curl -X "POST" "http://connectors:8083/connectors/" \
     -H "Content-Type: application/json" \
     -d '{
 "name": "kdgens",
  "config": {
    "connector.class": "io.confluent.kafka.connect.datagen.DatagenConnector",
    "kafka.topic": "users",
    "quickstart": "users",
    "key.converter": "org.apache.kafka.connect.storage.StringConverter",
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "value.converter.schemas.enable": "false",
    "max.interval": 300000,
    "iterations": 500,
    "tasks.max": "1"
  }
}'

# Properties file for internal client access from within K8s cluster
cat << EOF > kafka.properties
bootstrap.servers=kafka:9071
sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="test" password="test123";
sasl.mechanism=PLAIN
security.protocol=SASL_PLAINTEXT
EOF

# Properties file for external client access from a Docker container outside K8s cluster
cat << EOF > kafka.properties
bootstrap.servers=192.168.65.3:30220
sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="test" password="test123";
sasl.mechanism=PLAIN
security.protocol=SASL_PLAINTEXT
EOF

curl -X "POST" "http://connectors:8083/connectors/" \
    -H "Content-Type: application/json" \
    -d '{
              "name": "kddeb",
              "config": {
        "connector.class": "io.debezium.connector.mysql.MySqlConnector",
        "tasks.max": "1",
        "database.hostname": "kd-mysql.default.svc.cluster.local",
        "database.port": "3306",
        "database.user": "mysqluser",
        "database.password": "mysqlpw",
        "database.server.id": "223344",
        "database.server.name": "kdserver1",
        "database.include.list": "kddb",
        "database.history.kafka.bootstrap.servers": "kafka:9071",
        "database.history.kafka.topic": "mysql.kddb",
        "include.schema.changes": "false",
        "key.converter": "io.confluent.connect.avro.AvroConverter",
        "key.converter.schema.registry.url": "http://schemaregistry:8081",
        "value.converter": "io.confluent.connect.avro.AvroConverter",
        "value.converter.schema.registry.url": "http://schemaregistry:8081",
        "database.history.consumer.security.protocol": "SASL_PLAINTEXT",
        "database.history.consumer.sasl.mechanism": "PLAIN",
        "database.history.consumer.sasl.jaas.config": "org.apache.kafka.common.security.plain.PlainLoginModule required username=\"test\" password=\"test123\";",
        "database.history.producer.security.protocol": "SASL_PLAINTEXT",
        "database.history.producer.sasl.mechanism": "PLAIN",
        "database.history.producer.sasl.jaas.config": "org.apache.kafka.common.security.plain.PlainLoginModule required username=\"test\" password=\"test123\";"
       }
}'

curl -X "POST" "http://connectors:8083/connectors/" \
    -H "Content-Type: application/json" \
    -d '{
  "name": "kdes",
  "config": {
    "connector.class": "io.confluent.connect.elasticsearch.ElasticsearchSinkConnector",
    "key.converter": "org.apache.kafka.connect.storage.StringConverter",
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "transforms": "TimestampConverter",
    "topics": "users",
    "transforms.TimestampConverter.type": "org.apache.kafka.connect.transforms.TimestampConverter$Value",
    "transforms.TimestampConverter.target.type": "string",
    "transforms.TimestampConverter.field": "registertime",
    "transforms.TimestampConverter.format": "yyyy-MM-dd HH:mm:ss",
    "connection.url": "http://elasticsearch-master.default.svc.cluster.local:9200",
    "type.name": "_doc",
    "key.ignore": "true",
    "schema.ignore": "true",
    "value.converter.schemas.enable": "false"
    }
   }'

curl -X "POST" "http://connectors:8083/connectors/" \
    -H "Content-Type: application/json" \
    -d '{
  "name": "kdmqtt",
  "config": {
    "connector.class" : "io.confluent.connect.mqtt.MqttSourceConnector",
    "tasks.max" : "1",
    "mqtt.server.uri" : "tcp://kd-mosquitto.default.svc.cluster.local:1883",
    "mqtt.topics" : "temperature",
    "kafka.topic" : "mqtt.temperature",
    "value.converter": "org.apache.kafka.connect.converters.ByteArrayConverter",
    "confluent.topic.bootstrap.servers": "kafka:9071",
    "confluent.topic.security.protocol": "SASL_PLAINTEXT",
    "confluent.topic.sasl.mechanism": "PLAIN",
    "confluent.topic.sasl.jaas.config": "org.apache.kafka.common.security.plain.PlainLoginModule required username=\"test\" password=\"test123\";"
  }
}'

curl -X "POST" "http://connectors:8083/connectors/" \
     -H "Content-Type: application/json" \
     -d '{
 "name": "kdcust",
  "config": {
    "connector.class" : "com.kd.kdc.MySourceConnector",
    "tasks.max" : "1",
    "api.url": "http://junk.com:8080",
    "topic": "kdcust_topic",
    "sleep.seconds": 5
  }
}'

# Get list of schemas from schema registry
curl -X GET http://schemaregistry:8081/subjects/

# See the schema for a topic
curl --silent -X GET http://schemaregistry:8081/subjects/kdserver1.kddb.customer-value/versions/latest

# Check Avro messages from Kafka Connect container
export LOG_DIR=/opt
kafka-avro-console-consumer --topic kdserver1.kddb.customer --bootstrap-server kafka:9071 \
           --property schema.registry.url=http://schemaregistry:8081 --from-beginning --consumer.config kafka.properties

# Remove connector from K8s Kafka Connect
curl -XDELETE 'http://connectors:8083/connectors/kdgens'

# Install Custom Weather plugin on Dev Kafka Connect
confluent-hub install --no-prompt /kdweather/target/components/packages/kdoshi-myweather-1.0-SNAPSHOT.zip

# Create Custom Weather Connector on Dev Kafka Connect
curl -s -X PUT -H  "Content-Type:application/json" http://localhost:8083/connectors/kd-weather-01/config \
            -d '{
                  "connector.class" : "com.kd.kdw.MySourceConnector",
                  "tasks.max" : "1",
                  "key.converter": "com.kd.kdw.MyConverter",
                  "value.converter": "com.kd.kdw.MyConverter",
                  "cities": "Ireland, Peru, Brazil, Argentina, Chile",
                  "topic": "kdweather_topic",
                  "sleep.seconds": 60,
                  "transforms": "insertcurrenttime",
                  "transforms.insertcurrenttime.type": "com.kd.kdw.InsertCurrentTime$Value",
                  "transforms.insertcurrenttime.ct.field.name": "kdct"
        }'

# To format the Weather Connector messages as Json, add this to the config above
      "key.converter": "org.apache.kafka.connect.json.JsonConverter",
      "value.converter": "org.apache.kafka.connect.json.JsonConverter",

# Get connectors from Dev Kafka Connect
curl -X GET  http://localhost:8083/connectors/

# Custom Connector
mvn archetype:generate \
    -DarchetypeGroupId=com.github.jcustenborder.kafka.connect \
    -DarchetypeArtifactId=kafka-connect-quickstart \
    -DarchetypeVersion=2.4.0 \
    -Dpackage=com.kd.kdc \
    -DgroupId=com.kd \
    -DartifactId=kdconnector \
    -DpackageName=com.kd.kdc \
    -Dversion=1.0-SNAPSHOT

# Use VSCode Maven extension - From Command prompt, maven: Update Archetype catalog and then maven: Create New Project.
# Then select "More..." for remote archetypes and look for kafka-connect-quickstart from jcustenborder
C:\KD\Dev\bd\kafka\devp> & "C:\KD\Tools\maven-3.6.3\bin\mvn.cmd" \
    org.apache.maven.plugins:maven-archetype-plugin:3.1.2:generate \
    -DarchetypeArtifactId="kafka-connect-quickstart" \
    -DarchetypeGroupId="com.github.jcustenborder.kafka.connect" \
    -DarchetypeVersion="2.4.0"
Define value for property 'groupId': com.kd
Define value for property 'artifactId': kdconnector
Define value for property 'version' 1.0-SNAPSHOT: : 
Define value for property 'package' com.kd: : com.kd.kdc

[INFO] Parameter: basedir, Value: C:\KD\Dev\bd\kafka\devp
[INFO] Parameter: package, Value: com.kd.kdc
[INFO] Parameter: groupId, Value: com.kd
[INFO] Parameter: artifactId, Value: kdconnector
[INFO] Parameter: packageName, Value: com.kd.kdc
[INFO] Parameter: version, Value: 1.0-SNAPSHOT
[INFO] project created from Old (1.x) Archetype in dir: C:\KD\Dev\bd\kafka\devp\kdconnector

# Build Docker Connect image (with custom connector)
C:\KD\Dev\bd> docker build --tag=kd/kafkaconnect . -f .\helm\kd-confluent\Dockerfile.connect

# Create Package with Effective POM
"C:\KD\Tools\maven-3.6.3\bin\mvn.cmd" help:effective-pom -f "c:\KD\Dev\bd\kafka\devp\kdconnector\pom.xml" >kafka\devp\kdconnector\effective.xml
"C:\KD\Tools\maven-3.6.3\bin\mvn.cmd" package -f "c:\KD\Dev\bd\kafka\devp\kdconnector\effective.xml"

# Maven builds
"C:\KD\Tools\maven-3.6.3\bin\mvn.cmd" clean -f "c:\KD\Dev\bd\kafka\devp\kdtest\pom.xml"
cmd /c "C:\KD\Tools\maven-3.6.3\bin\mvn.cmd" package -f "c:\kd\Dev\bd\kafka\devp\kdtest\foo.xml"

# Launch Spark application kafkaavro
kubectl delete sparkapplication kd-kafkaavro
kubectl apply -f /kd-spark/kd-kafkaavro.yaml
