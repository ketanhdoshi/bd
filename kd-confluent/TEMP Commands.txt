#-------------------------------------------
# Setup Kubernetes Dashboard
#-------------------------------------------
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.2.0/aio/deploy/recommended.yaml
kubectl apply -f k8s/dashboard-adminuser.yaml

# Increase login token timeout. After running the command below, a notepad window opens. Find the container
# section and then add "- --token-ttl=43200" under the "args:". Save and exit notepad.
kubectl edit deployment -n kubernetes-dashboard kubernetes-dashboard

# Get K8s token and open dashboard in browser
kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | sls admin-user | ForEach-Object { $_ -Split '\s+' } | Select -First 1)
http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/

#-------------------------------------------
# Setup MySQL with Helm in namespace 'mysql'
#-------------------------------------------
kubectl apply -f mysql\mysql-pv.yaml
kubectl create namespace mysql
C:\kd\Tools\helm\helm.exe install kd-mysql -n mysql --set mysqlRootPassword=debezium --set mysqlUser=mysqluser --set mysqlPassword=mysqlpw --set mysqlDatabase=kddb -f .\mysql\mysql-values.yaml stable/mysql

#-------------------------------------------
# Access mysql from an interactive shell on the mysqlcli Docker container
#-------------------------------------------
mysql -h 192.168.65.3 -P30300 -D kddb -u mysqluser -pmysqlpw
MySQL [kddb]> select * from customer;

#-------------------------------------------
# Setup ElasticSearch with Helm in namespace 'kd-es'
#-------------------------------------------
C:\KD\Tools\helm\helm repo add elastic https://helm.elastic.co
c:\kd\tools\helm\helm repo update
kubectl apply -f es/es-pv.yaml
kubectl create namespace kd-es
c:\kd\tools\helm\helm install -f es/es-values.yaml elasticsearch -n kd-es --version 7.9.3 elastic/elasticsearch
c:\kd\tools\helm\helm install kibana --version 7.9.3 elastic/kibana  -n kd-es -f es/kibana-values.yaml

#-------------------------------------------
# Setup Mosquitto with Helm in namespace 'kd-mqtt'
#-------------------------------------------
C:\KD\Tools\helm\helm repo add t3n https://storage.googleapis.com/t3n-helm-charts
C:\KD\Tools\helm\helm search repo mosquitto
c:\kd\tools\helm\helm repo update
kubectl apply -f ./mosquitto/mqtt-pv.yaml
kubectl create namespace kd-mqtt
c:\kd\tools\helm\helm install kd-mosquitto t3n/mosquitto --version 2.2.0 -f .\mosquitto\mqtt-values.yaml -n kd-mqtt

# Test Mosquitto from two separate terminals on the Mosquitto container
mosquitto_sub -h kd-mosquitto -t dummy -C 5
mosquitto_pub -h kd-mosquitto -t dummy -m "Hello world again"

#-------------------------------------------
# Setup Confluent with Helm in namespace 'kd-confluent'
#-------------------------------------------
kubectl create namespace kd-confluent
# NB: Since the Persistent Volumes are all the same size, there is a chance that the wrong ones will get bound to
# the Persistent Volume Claims. eg. the Control Center PV gets bound to the KSQL PVC. So rather than creating all
# the PVs at once, we might have to create the PVs one at a time, then helm install the corresponding component, and 
# then create the next PV.
kubectl apply -f .\kd-confluent\kd-pv.yaml
cd helm/confluent/helm
c:\kd\tools\helm\helm upgrade --install  operator  ./confluent-operator  --values ..\..\..\kd-confluent\kd-values.yaml --namespace kd-confluent  --set operator.enabled=true
c:\kd\tools\helm\helm upgrade --install  zookeeper ./confluent-operator  --values ..\..\..\kd-confluent\kd-values.yaml --namespace kd-confluent  --set zookeeper.enabled=true
c:\kd\tools\helm\helm upgrade --install  kafka ./confluent-operator  --values ..\..\..\kd-confluent\kd-values.yaml --namespace kd-confluent  --set kafka.enabled=true
c:\kd\tools\helm\helm upgrade --install  schemaregistry ./confluent-operator  --values ..\..\..\kd-confluent\kd-values.yaml --namespace kd-confluent  --set schemaregistry.enabled=true
c:\kd\tools\helm\helm upgrade --install  connectors ./confluent-operator  --values ..\..\..\kd-confluent\kd-values.yaml --namespace kd-confluent  --set connect.enabled=true
c:\kd\tools\helm\helm upgrade --install  controlcenter ./confluent-operator  --values ..\..\..\kd-confluent\kd-values.yaml --namespace kd-confluent  --set controlcenter.enabled=true
c:\kd\tools\helm\helm upgrade --install  ksql ./confluent-operator  --values ..\..\..\kd-confluent\kd-values.yaml --namespace kd-confluent  --set ksql.enabled=true

# Set up a Kafkacat client
cd ../../..
kubectl apply -f ./kd-confluent/kafkacat.yaml -n kd-confluent

#-------------------------------------------
# Setup Connectors
#-------------------------------------------
# Debezium MySql
kubectl -n kd-confluent apply -f .\kd-confluent\connect-mysql.yaml

# Mosquitto MQTT
kubectl -n kd-confluent apply -f .\kd-confluent\connect-mqtt.yaml

#-------------------------------------------
# Setup Ingress
#-------------------------------------------
c:\kd\tools\helm\helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
c:\kd\tools\helm\helm repo update
c:\kd\tools\helm\helm.exe install ingress ingress-nginx/ingress-nginx -n kd-confluent
kubectl apply -f .\kd-confluent\kd-ingress.yaml

# Get the Ingress port
kubectl --namespace kd-confluent get services -o wide -w ingress-ingress-nginx-controller

#-------------------------------------------
# Setup Spark
#-------------------------------------------
kubectl create namespace spark-app
c:\kd\tools\helm\helm repo add spark-operator https://googlecloudplatform.github.io/spark-on-k8s-operator
c:\kd\tools\helm\helm repo update
c:\kd\tools\helm\helm install sparkop spark-operator/spark-operator --namespace spark-operator --create-namespace --set sparkJobNamespace=spark-app --set webhook.enable=true --set image.tag=latest

#-------------------------------------------
# Run Spark examples
#-------------------------------------------
kubectl delete sparkapplication spark-pi -n spark-app
kubectl apply -f ./kd-spark/spark-pi.yaml

kubectl delete sparkapplication pyspark-pi  -n spark-app
kubectl apply -f ./kd-spark/spark-python.yaml

kubectl apply -f ./kd-spark/kd-py-simple.yaml 
kubectl delete sparkapplication kd-py-simple

kubectl apply -f ./kd-spark/kd-py-channel.yaml   
kubectl delete sparkapplication kd-py-channel

kubectl apply -f ./kd-spark/kd-py-channel-kafka.yaml
kubectl delete sparkapplication kd-py-channel-kafka

#-------------------------------------------
# Kafkacat commands
#-------------------------------------------

# Internal Kafkacat within kd-confluent namespace - list topics, produce and consume messages
kafkacat -b kafka:9071 -L -F /etc/kafka-client-properties/kafka-client.properties
kafkacat -b kafka:9071 -t qe -P -F /etc/kafka-client-properties/kafka-client.properties
kafkacat -b kafka:9071 -t qe -C -F /etc/kafka-client-properties/kafka-client.properties

# Get Connector offsets
kafkacat -b kafka:9071-C -t kd-confluent.connectors-offsets -f 'Partition(%p) %k %s\n' -F /etc/kafka-client-properties/kafka-client.properties
# Partition(8) ["kddeb",{"server":"kdserver1"}] {"file":"mysql-bin.000024","pos":154}
# Partition(8) ["kddeb",{"server":"kdserver1"}] {"ts_sec":1609739412,"file":"mysql-bin.000024","pos":219,"row":1,"server_id":223344,"event":2}
# Partition(8) ["kddeb",{"server":"kdserver1"}] {"ts_sec":1609740058,"file":"mysql-bin.000024","pos":501,"row":1,"server_id":223344,"event":2}
# Partition(8) ["kddeb",{"server":"kdserver1"}] {"ts_sec":1609751757,"file":"mysql-bin.000024","pos":785,"row":1,"server_id":223344,"event":2}

# Reset Connector offsets for a specific connector for partition 8
echo '["kddeb",{"server":"kdserver1"}]|' | \
kafkacat -P -Z -b kafka:9071 -t kd-confluent.connectors-offsets -K \| -p 8 -F /etc/kafka-client-properties/kafka-client.properties

# Produce with key. So input <key>:<value>
kafkacat -b kafka:9071 -t qe -P -F /etc/kafka-client-properties/kafka-client.properties

# Internal Kafkacat outside kd-confluent namespace - list topics, produce and consume messages
kafkacat -b kafka.kd-confluent.svc.cluster.local:9071 -L -F /etc/kafka-client-properties/kafka-client.properties

# External Kafkacat
kafkacat -b 192.168.65.3:30220 -L -F kafka.properties

#-------------------------------------------
# Helm Commands
#-------------------------------------------
# List Repos and update repos
C:\kd\Tools\helm\helm.exe repo list
C:\kd\Tools\helm\helm.exe repo update

#-------------------------------------------
# Run Connectors on Dev Kafka container (ie. on KDWeather container)
#-------------------------------------------
# Install Datagen connector
confluent-hub install --no-prompt confluentinc/kafka-connect-datagen:0.4.0

# Start the Kafka Connect server
/etc/confluent/docker/run &

# Configure Datagen Connector for Ads data 
curl -X PUT http://localhost:8083/connectors/kd-ads-01/config \
    -s -H  "Content-Type:application/json" \
    -d '{
        "connector.class": "io.confluent.kafka.connect.datagen.DatagenConnector",
        "kafka.topic": "testad1",
        "schema.filename": "/kdweather/ads.avro",
        "schema.keyfield": "ad_id",
        "value.converter.schemas.enable": "false",
        "key.converter": "org.apache.kafka.connect.storage.StringConverter",
        "value.converter": "org.apache.kafka.connect.json.JsonConverter",
        "max.interval": 7500,
        "iterations": 5,
        "tasks.max": "1"
    }'

# List all installed connectors from Dev Kafka Connect
curl -X GET  http://localhost:8083/connectors/

# Delete Datagen Connector
curl -XDELETE 'http://localhost:8083/connectors/kd-ads-01'

#-------- custom KDWeather Connector ----

# Install Custom Weather plugin for OpenWeather REST API data on Dev Kafka Connect
confluent-hub install --no-prompt /kdweather/target/components/packages/kdoshi-myweather-1.0-SNAPSHOT.zip

# Create Custom Weather Connector on Dev Kafka Connect
curl -s -X PUT -H  "Content-Type:application/json" http://localhost:8083/connectors/kd-weather-01/config \
            -d '{
                  "connector.class" : "com.kd.kdw.MySourceConnector",
                  "tasks.max" : "1",
                  "key.converter": "com.kd.kdw.MyConverter",
                  "value.converter": "com.kd.kdw.MyConverter",
                  "cities": "Ireland, Peru, Brazil, Argentina, Chile",
                  "topic": "kdweather_topic",
                  "sleep.seconds": 60,
                  "transforms": "insertcurrenttime",
                  "transforms.insertcurrenttime.type": "com.kd.kdw.InsertCurrentTime$Value",
                  "transforms.insertcurrenttime.ct.field.name": "kdct"
        }'

# To format the Weather Connector messages as Json, add this to the config above
      "key.converter": "org.apache.kafka.connect.json.JsonConverter",
      "value.converter": "org.apache.kafka.connect.json.JsonConverter",

# Delete Weather Connector
curl -XDELETE 'http://localhost:8083/connectors/kd-weather-01'

#-------------------------------------------
# MISC
#-------------------------------------------

# Confluent Control Center port forward and UI
kubectl port-forward pod/controlcenter-0  9021:9021 -n kd-confluent
http://localhost:9021

# Kibana port forward and UI
kubectl port-forward svc/kibana-kibana 5601
http://localhost:5601

# Get logs from Connector container
docker logs c919509dc787d0a0e8a585fcfdc909cc670e3d1988f9a9421a41a7f24a5f71ec 2>&1 >kdccc.log
docker logs 1ad1de2bec4351c192abf3e52f6899cc6b4cd59a2fad539cb6fbd6950bb08303 2>&1 >kdkfk.log



# On Mosquitto container - publish and consume messages
mosquitto_pub -h kd-mosquitto -t temperature -m "good morning"
mosquitto_sub -h kd-mosquitto -t temperature -C 5

# Configure Datagen connector
curl -X "POST" "http://connectors:8083/connectors/" \
     -H "Content-Type: application/json" \
     -d '{
 "name": "kdgens",
  "config": {
    "connector.class": "io.confluent.kafka.connect.datagen.DatagenConnector",
    "kafka.topic": "users",
    "quickstart": "users",
    "key.converter": "org.apache.kafka.connect.storage.StringConverter",
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "value.converter.schemas.enable": "false",
    "max.interval": 300000,
    "iterations": 500,
    "tasks.max": "1"
  }
}'

# Properties file for internal client access from within K8s cluster
cat << EOF > kafka.properties
bootstrap.servers=kafka:9071
sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="test" password="test123";
sasl.mechanism=PLAIN
security.protocol=SASL_PLAINTEXT
EOF

# Properties file for external client access from a Docker container outside K8s cluster
cat << EOF > kafka.properties
bootstrap.servers=192.168.65.3:30220
sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="test" password="test123";
sasl.mechanism=PLAIN
security.protocol=SASL_PLAINTEXT
EOF

curl -X "POST" "http://connectors:8083/connectors/" \
    -H "Content-Type: application/json" \
    -d '{
              "name": "kddeb",
              "config": {
        "connector.class": "io.debezium.connector.mysql.MySqlConnector",
        "tasks.max": "1",
        "database.hostname": "kd-mysql.default.svc.cluster.local",
        "database.port": "3306",
        "database.user": "mysqluser",
        "database.password": "mysqlpw",
        "database.server.id": "223344",
        "database.server.name": "kdserver1",
        "database.include.list": "kddb",
        "database.history.kafka.bootstrap.servers": "kafka:9071",
        "database.history.kafka.topic": "mysql.kddb",
        "include.schema.changes": "false",
        "key.converter": "io.confluent.connect.avro.AvroConverter",
        "key.converter.schema.registry.url": "http://schemaregistry:8081",
        "value.converter": "io.confluent.connect.avro.AvroConverter",
        "value.converter.schema.registry.url": "http://schemaregistry:8081",
        "database.history.consumer.security.protocol": "SASL_PLAINTEXT",
        "database.history.consumer.sasl.mechanism": "PLAIN",
        "database.history.consumer.sasl.jaas.config": "org.apache.kafka.common.security.plain.PlainLoginModule required username=\"test\" password=\"test123\";",
        "database.history.producer.security.protocol": "SASL_PLAINTEXT",
        "database.history.producer.sasl.mechanism": "PLAIN",
        "database.history.producer.sasl.jaas.config": "org.apache.kafka.common.security.plain.PlainLoginModule required username=\"test\" password=\"test123\";"
       }
}'

curl -X "POST" "http://connectors:8083/connectors/" \
    -H "Content-Type: application/json" \
    -d '{
  "name": "kdes",
  "config": {
    "connector.class": "io.confluent.connect.elasticsearch.ElasticsearchSinkConnector",
    "key.converter": "org.apache.kafka.connect.storage.StringConverter",
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "transforms": "TimestampConverter",
    "topics": "users",
    "transforms.TimestampConverter.type": "org.apache.kafka.connect.transforms.TimestampConverter$Value",
    "transforms.TimestampConverter.target.type": "string",
    "transforms.TimestampConverter.field": "registertime",
    "transforms.TimestampConverter.format": "yyyy-MM-dd HH:mm:ss",
    "connection.url": "http://elasticsearch-master.default.svc.cluster.local:9200",
    "type.name": "_doc",
    "key.ignore": "true",
    "schema.ignore": "true",
    "value.converter.schemas.enable": "false"
    }
   }'

curl -X "POST" "http://connectors:8083/connectors/" \
    -H "Content-Type: application/json" \
    -d '{
  "name": "kdmqtt",
  "config": {
    "connector.class" : "io.confluent.connect.mqtt.MqttSourceConnector",
    "tasks.max" : "1",
    "mqtt.server.uri" : "tcp://kd-mosquitto.default.svc.cluster.local:1883",
    "mqtt.topics" : "temperature",
    "kafka.topic" : "mqtt.temperature",
    "value.converter": "org.apache.kafka.connect.converters.ByteArrayConverter",
    "confluent.topic.bootstrap.servers": "kafka:9071",
    "confluent.topic.security.protocol": "SASL_PLAINTEXT",
    "confluent.topic.sasl.mechanism": "PLAIN",
    "confluent.topic.sasl.jaas.config": "org.apache.kafka.common.security.plain.PlainLoginModule required username=\"test\" password=\"test123\";"
  }
}'

curl -X "POST" "http://connectors:8083/connectors/" \
     -H "Content-Type: application/json" \
     -d '{
 "name": "kdcust",
  "config": {
    "connector.class" : "com.kd.kdc.MySourceConnector",
    "tasks.max" : "1",
    "api.url": "http://junk.com:8080",
    "topic": "kdcust_topic",
    "sleep.seconds": 5
  }
}'

# Get list of schemas from schema registry
curl -X GET http://schemaregistry:8081/subjects/

# See the schema for a topic
curl --silent -X GET http://schemaregistry:8081/subjects/kdserver1.kddb.customer-value/versions/latest

# Check Avro messages from Kafka Connect container
export LOG_DIR=/opt
kafka-avro-console-consumer --topic kdserver1.kddb.customer --bootstrap-server kafka:9071 \
           --property schema.registry.url=http://schemaregistry:8081 --from-beginning --consumer.config kafka.properties

# Remove connector from K8s Kafka Connect
curl -XDELETE 'http://connectors:8083/connectors/kdgens'



# Custom Connector
mvn archetype:generate \
    -DarchetypeGroupId=com.github.jcustenborder.kafka.connect \
    -DarchetypeArtifactId=kafka-connect-quickstart \
    -DarchetypeVersion=2.4.0 \
    -Dpackage=com.kd.kdc \
    -DgroupId=com.kd \
    -DartifactId=kdconnector \
    -DpackageName=com.kd.kdc \
    -Dversion=1.0-SNAPSHOT

# Use VSCode Maven extension - From Command prompt, maven: Update Archetype catalog and then maven: Create New Project.
# Then select "More..." for remote archetypes and look for kafka-connect-quickstart from jcustenborder
C:\KD\Dev\bd\kafka\devp> & "C:\KD\Tools\maven-3.6.3\bin\mvn.cmd" \
    org.apache.maven.plugins:maven-archetype-plugin:3.1.2:generate \
    -DarchetypeArtifactId="kafka-connect-quickstart" \
    -DarchetypeGroupId="com.github.jcustenborder.kafka.connect" \
    -DarchetypeVersion="2.4.0"
Define value for property 'groupId': com.kd
Define value for property 'artifactId': kdconnector
Define value for property 'version' 1.0-SNAPSHOT: : 
Define value for property 'package' com.kd: : com.kd.kdc

[INFO] Parameter: basedir, Value: C:\KD\Dev\bd\kafka\devp
[INFO] Parameter: package, Value: com.kd.kdc
[INFO] Parameter: groupId, Value: com.kd
[INFO] Parameter: artifactId, Value: kdconnector
[INFO] Parameter: packageName, Value: com.kd.kdc
[INFO] Parameter: version, Value: 1.0-SNAPSHOT
[INFO] project created from Old (1.x) Archetype in dir: C:\KD\Dev\bd\kafka\devp\kdconnector

# Build Docker Connect image (with custom connector)
C:\KD\Dev\bd> docker build --tag=kd/kafkaconnect . -f .\helm\kd-confluent\Dockerfile.connect

# Create Package with Effective POM
"C:\KD\Tools\maven-3.6.3\bin\mvn.cmd" help:effective-pom -f "c:\KD\Dev\bd\kafka\devp\kdconnector\pom.xml" >kafka\devp\kdconnector\effective.xml
"C:\KD\Tools\maven-3.6.3\bin\mvn.cmd" package -f "c:\KD\Dev\bd\kafka\devp\kdconnector\effective.xml"

# Maven builds
"C:\KD\Tools\maven-3.6.3\bin\mvn.cmd" clean -f "c:\KD\Dev\bd\kafka\devp\kdtest\pom.xml"
cmd /c "C:\KD\Tools\maven-3.6.3\bin\mvn.cmd" package -f "c:\kd\Dev\bd\kafka\devp\kdtest\foo.xml"

# Launch Spark application kafkaavro
kubectl delete sparkapplication kd-kafkaavro
kubectl apply -f /kd-spark/kd-kafkaavro.yaml

# Configure Datagen connector to generate Events data (for Spark integration kafkaint)
curl -X PUT http://localhost:8083/connectors/kd-events-01/config \
    -s -H  "Content-Type:application/json" \
    -d '{
        "connector.class": "io.confluent.kafka.connect.datagen.DatagenConnector",
        "kafka.topic": "test1",
        "schema.filename": "/kdweather/events.avro",
        "schema.keyfield": "event_time",
        "key.converter": "org.apache.kafka.connect.storage.StringConverter",
        "value.converter": "io.confluent.connect.avro.AvroConverter",
        "value.converter.schema.registry.url": "http://192.168.65.3:30075",
        "max.interval": 7500,
        "iterations": 5,
        "tasks.max": "1"
    }'
